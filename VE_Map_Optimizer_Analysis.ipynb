{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe71b0cd",
   "metadata": {},
   "source": [
    "# üîß Configura√ß√£o do Ambiente WSL + VS Code\n",
    "\n",
    "## ‚úÖ Inserir URL do Servidor Jupyter Manualmente\n",
    "\n",
    "### Passo 1: Obter a URL do servidor\n",
    "No terminal WSL que est√° rodando o Jupyter, procure por uma linha similar a:\n",
    "```\n",
    "http://127.0.0.1:8888/lab?token=abc123def456...\n",
    "```\n",
    "\n",
    "### Passo 2: Configurar no VS Code\n",
    "1. **Command Palette** (`Ctrl+Shift+P`)\n",
    "2. Digite: **\"Jupyter: Specify Jupyter Server for Connections\"**\n",
    "3. Selecione **\"Existing\"**\n",
    "4. Cole a URL completa com token\n",
    "\n",
    "### Passo 3: Selecionar o Kernel\n",
    "1. No notebook, clique no seletor de kernel (canto superior direito)\n",
    "2. Escolha **\"Existing Jupyter Server\"**\n",
    "3. Selecione o servidor que voc√™ acabou de configurar\n",
    "\n",
    "### üîç URL Base para WSL:\n",
    "- **Localhost**: `http://127.0.0.1:8888`\n",
    "- **WSL IP**: `http://172.x.x.x:8888` (se localhost n√£o funcionar)\n",
    "\n",
    "### üõ†Ô∏è Troubleshooting:\n",
    "- Se 127.0.0.1 n√£o funcionar, use o IP do WSL: `ip addr show eth0`\n",
    "- Token est√° sempre na sa√≠da do comando `jupyter lab`\n",
    "- Porta padr√£o: 8888\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f79b156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Informa√ß√µes de rede do WSL:\n",
      "==================================================\n",
      "üìç IP WSL: 172.29.35.51\n",
      "üåê Localhost: 127.0.0.1\n",
      "üö™ Porta Jupyter padr√£o: 8888\n",
      "\n",
      "üìã URLs para testar:\n",
      "‚Ä¢ http://127.0.0.1:8888\n",
      "‚Ä¢ http://172.29.35.51:8888\n",
      "\n",
      "üí° Dica: O token aparece na sa√≠da do comando 'jupyter lab' no terminal\n",
      "üìù Formato da URL: http://IP:8888/lab?token=SEU_TOKEN_AQUI\n"
     ]
    }
   ],
   "source": [
    "# üåê Informa√ß√µes de Rede para Conex√£o Manual\n",
    "import subprocess\n",
    "import socket\n",
    "\n",
    "print(\"üîç Informa√ß√µes de rede do WSL:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# IP do WSL\n",
    "try:\n",
    "    result = subprocess.run(['ip', 'addr', 'show', 'eth0'], capture_output=True, text=True)\n",
    "    lines = result.stdout.split('\\n')\n",
    "    for line in lines:\n",
    "        if 'inet ' in line and '127.0.0.1' not in line:\n",
    "            ip = line.strip().split()[1].split('/')[0]\n",
    "            print(f\"üìç IP WSL: {ip}\")\n",
    "            break\n",
    "except:\n",
    "    print(\"‚ùå N√£o foi poss√≠vel obter IP do WSL\")\n",
    "\n",
    "# Portas em uso\n",
    "print(f\"üåê Localhost: 127.0.0.1\")\n",
    "print(f\"üö™ Porta Jupyter padr√£o: 8888\")\n",
    "print()\n",
    "\n",
    "print(\"üìã URLs para testar:\")\n",
    "print(\"‚Ä¢ http://127.0.0.1:8888\")\n",
    "try:\n",
    "    ip = subprocess.run(['hostname', '-I'], capture_output=True, text=True).stdout.strip().split()[0]\n",
    "    print(f\"‚Ä¢ http://{ip}:8888\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print()\n",
    "print(\"üí° Dica: O token aparece na sa√≠da do comando 'jupyter lab' no terminal\")\n",
    "print(\"üìù Formato da URL: http://IP:8888/lab?token=SEU_TOKEN_AQUI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do ambiente Python\n",
    "import sys\n",
    "print(f\"üêç Python execut√°vel: {sys.executable}\")\n",
    "print(f\"üìç Caminho Python: {sys.path[0]}\")\n",
    "print(f\"üî¢ Vers√£o Python: {sys.version}\")\n",
    "\n",
    "# Teste simples de importa√ß√£o\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"‚úÖ Pandas importado com sucesso! Vers√£o: {pd.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erro ao importar pandas: {e}\")\n",
    "    print(f\"\udca1 Verifique se o kernel correto est√° selecionado: 'OBD2 VE Optimizer'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef549d",
   "metadata": {},
   "source": [
    "# Otimiza√ß√£o de Mapa VE com IA - An√°lise de Inje√ß√£o Eletr√¥nica\n",
    "\n",
    "Este notebook implementa um modelo de Machine Learning para otimizar mapas de efici√™ncia volum√©trica (VE) baseado em dados de sensor O2 wideband.\n",
    "\n",
    "## Especifica√ß√µes do Motor\n",
    "- **Cilindrada**: 2000cc\n",
    "- **Cilindros**: 4\n",
    "- **Bicos Injetores**: 4 x 19lb/h\n",
    "- **Modo de Inje√ß√£o**: Banco a banco (2 inje√ß√µes por ciclo por banco)\n",
    "\n",
    "## Objetivos\n",
    "1. Estimar o delay da sonda O2 no escapamento\n",
    "2. Analisar erros entre lambda medido vs target\n",
    "3. Treinar modelo de IA para prever ajustes no mapa VE\n",
    "4. Gerar nova tabela VE otimizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10a48ff",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b5e64ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Imports necess√°rios\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Imports necess√°rios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy import interpolate, signal\n",
    "from scipy.optimize import minimize\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
    "print(f\"üìÖ An√°lise iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Especifica√ß√µes do motor\n",
    "ENGINE_SPECS = {\n",
    "    'displacement': 2000,  # cc\n",
    "    'cylinders': 4,\n",
    "    'injectors': 4,\n",
    "    'injector_flow': 19,  # lb/h\n",
    "    'injection_mode': 'bank_to_bank',\n",
    "    'stoich_afr': 14.7  # AFR estequiom√©trico para gasolina\n",
    "}\n",
    "\n",
    "print(\"\\nüöó Especifica√ß√µes do Motor:\")\n",
    "for key, value in ENGINE_SPECS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b790775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o melhorada para carregar dados do log (incluindo MLG)\n",
    "import struct\n",
    "\n",
    "class MLGLogReader:\n",
    "    \"\"\"Leitor de arquivos MLG (MegaLogViewer) do TunerStudio\"\"\"\n",
    "    \n",
    "    def read_mlg_file(self, file_path):\n",
    "        \"\"\"L√™ arquivo MLG bin√°rio e extrai dados\"\"\"\n",
    "        try:\n",
    "            print(f\"üìÇ Lendo arquivo MLG: {file_path}\")\n",
    "            \n",
    "            with open(file_path, 'rb') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "                # Procura por padr√µes de dados conhecidos\n",
    "                data_start = self._find_data_section(content)\n",
    "                if data_start == -1:\n",
    "                    print(\"‚ùå N√£o foi poss√≠vel encontrar se√ß√£o de dados\")\n",
    "                    return None\n",
    "                \n",
    "                # Extrai dados usando heur√≠stica\n",
    "                df = self._extract_data_heuristic(content, data_start)\n",
    "                \n",
    "                if df is not None:\n",
    "                    print(f\"‚úÖ Dados MLG extra√≠dos: {len(df)} registros\")\n",
    "                    return df\n",
    "                else:\n",
    "                    print(\"‚ùå Falha na extra√ß√£o de dados MLG\")\n",
    "                    return None\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao ler MLG: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _find_data_section(self, content):\n",
    "        \"\"\"Encontra in√≠cio da se√ß√£o de dados num√©ricos\"\"\"\n",
    "        patterns = [b'Time', b'RPM', b'Lambda', b'TPS']\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            pos = content.find(pattern)\n",
    "            if pos != -1:\n",
    "                for i in range(pos, min(pos + 10000, len(content) - 4), 4):\n",
    "                    try:\n",
    "                        val = struct.unpack('<f', content[i:i+4])[0]\n",
    "                        if 500 < val < 8000:  # Poss√≠vel RPM\n",
    "                            return i\n",
    "                    except:\n",
    "                        continue\n",
    "        return -1\n",
    "    \n",
    "    def _extract_data_heuristic(self, content, start_pos):\n",
    "        \"\"\"Extrai dados usando heur√≠stica baseada em valores t√≠picos\"\"\"\n",
    "        try:\n",
    "            estimated_channels = 14\n",
    "            remaining_bytes = len(content) - start_pos\n",
    "            record_size = estimated_channels * 4\n",
    "            num_records = remaining_bytes // record_size\n",
    "            \n",
    "            if num_records < 100:\n",
    "                return None\n",
    "            \n",
    "            num_records = min(num_records, 50000)\n",
    "            \n",
    "            data_matrix = []\n",
    "            pos = start_pos\n",
    "            \n",
    "            for i in range(num_records):\n",
    "                if pos + record_size > len(content):\n",
    "                    break\n",
    "                \n",
    "                record = []\n",
    "                for j in range(estimated_channels):\n",
    "                    if pos + 4 <= len(content):\n",
    "                        try:\n",
    "                            val = struct.unpack('<f', content[pos:pos+4])[0]\n",
    "                            record.append(val)\n",
    "                            pos += 4\n",
    "                        except:\n",
    "                            record.append(np.nan)\n",
    "                            pos += 4\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                if len(record) == estimated_channels:\n",
    "                    data_matrix.append(record)\n",
    "            \n",
    "            columns = ['RPM', 'COOLANT_TEMP', 'MAF', 'THROTTLE_POS', 'INTAKE_TEMP', \n",
    "                      'TIMING_ADVANCE', 'ENGINE_LOAD', 'ELM_VOLTAGE', 'SPEED', \n",
    "                      'O2_S1_WR_CURRENT', 'O2_S5_WR_CURRENT', 'O2_B2S2', \n",
    "                      'SHORT_FUEL_TRIM_1', 'SHORT_FUEL_TRIM_2']\n",
    "            \n",
    "            df = pd.DataFrame(data_matrix, columns=columns[:len(data_matrix[0])])\n",
    "            \n",
    "            # Filtra valores absurdos\n",
    "            df = self._clean_extracted_data(df)\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na extra√ß√£o heur√≠stica: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _clean_extracted_data(self, df):\n",
    "        \"\"\"Limpa dados extra√≠dos removendo valores absurdos\"\"\"\n",
    "        valid_ranges = {\n",
    "            'RPM': (300, 8000),\n",
    "            'COOLANT_TEMP': (0, 150),\n",
    "            'THROTTLE_POS': (0, 100),\n",
    "            'INTAKE_TEMP': (-20, 80),\n",
    "            'TIMING_ADVANCE': (-10, 60),\n",
    "            'ENGINE_LOAD': (0, 150),\n",
    "            'ELM_VOLTAGE': (10, 16),\n",
    "            'SPEED': (0, 300),\n",
    "            'O2_B2S2': (0.4, 1.5)\n",
    "        }\n",
    "        \n",
    "        initial_count = len(df)\n",
    "        \n",
    "        for col, (min_val, max_val) in valid_ranges.items():\n",
    "            if col in df.columns:\n",
    "                mask = (df[col] >= min_val) & (df[col] <= max_val)\n",
    "                df = df[mask]\n",
    "        \n",
    "        print(f\"üßπ Limpeza MLG: {initial_count} -> {len(df)} registros ({len(df)/initial_count*100:.1f}% retidos)\")\n",
    "        \n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "def load_ecu_log_data_advanced(file_path):\n",
    "    \"\"\"Carrega e processa dados do log da ECU com filtros de qualidade\"\"\"\n",
    "    try:\n",
    "        # Determina tipo de arquivo\n",
    "        if file_path.endswith('.mlg'):\n",
    "            reader = MLGLogReader()\n",
    "            df = reader.read_mlg_file(file_path)\n",
    "            if df is None:\n",
    "                return None\n",
    "        else:\n",
    "            # Arquivo CSV/texto\n",
    "            columns = ['RPM', 'COOLANT_TEMP', 'MAF', 'THROTTLE_POS', 'INTAKE_TEMP', \n",
    "                      'TIMING_ADVANCE', 'ENGINE_LOAD', 'ELM_VOLTAGE', 'SPEED', \n",
    "                      'O2_S1_WR_CURRENT', 'O2_S5_WR_CURRENT', 'O2_B2S2', \n",
    "                      'SHORT_FUEL_TRIM_1', 'SHORT_FUEL_TRIM_2']\n",
    "            \n",
    "            df = pd.read_csv(file_path, names=columns)\n",
    "            print(f\"üìä Log texto carregado: {len(df)} registros\")\n",
    "        \n",
    "        # Adiciona campos calculados\n",
    "        df['Time'] = np.arange(len(df)) * 0.1\n",
    "        df['Lambda'] = df['O2_B2S2'] / ENGINE_SPECS['stoich_afr']\n",
    "        df['FuelLoad'] = df['ENGINE_LOAD']\n",
    "        \n",
    "        # Taxa de mudan√ßa do TPS (suavizada)\n",
    "        df['TPS_Rate'] = df['THROTTLE_POS'].diff() / 0.1\n",
    "        df['TPS_Rate'] = df['TPS_Rate'].rolling(window=3, center=True).mean()\n",
    "        df['TPS_Rate'] = df['TPS_Rate'].fillna(0)\n",
    "        \n",
    "        # Estimativa de PW (pulso dos bicos)\n",
    "        df['PW_Estimated'] = (df['ENGINE_LOAD'] * df['RPM'] * 2.0) / (19 * 14.7 * 10000)\n",
    "        df['PW_Estimated'] = np.clip(df['PW_Estimated'], 0, 20)\n",
    "        \n",
    "        # Detecta fuel cut (DFCO)\n",
    "        df['Is_FuelCut'] = (df['PW_Estimated'] < 1.0) | (df['Lambda'] > 1.1)\n",
    "        \n",
    "        print(f\"\udcc8 Campos calculados adicionados\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar log: {e}\")\n",
    "        return None\n",
    "\n",
    "# Testa carregamento com arquivo MLG maior\n",
    "mlg_file = '/home/robca/obd2/logs/2025-08-06_00.09.39.mlg'\n",
    "csv_file = '/home/robca/obd2/logs/example.log'\n",
    "\n",
    "print(\"üîç Testando leitura de arquivos...\")\n",
    "\n",
    "# Testa MLG\n",
    "df_mlg = load_ecu_log_data_advanced(mlg_file)\n",
    "if df_mlg is not None:\n",
    "    print(f\"\\nüìä Dados MLG - Estat√≠sticas b√°sicas:\")\n",
    "    print(df_mlg[['RPM', 'Lambda', 'FuelLoad', 'THROTTLE_POS']].describe().round(3))\n",
    "\n",
    "# Testa CSV (refer√™ncia)\n",
    "df_csv = load_ecu_log_data_advanced(csv_file)\n",
    "if df_csv is not None:\n",
    "    print(f\"\\n\udcca Dados CSV - Estat√≠sticas b√°sicas:\")\n",
    "    print(df_csv[['RPM', 'Lambda', 'FuelLoad', 'THROTTLE_POS']].describe().round(3))\n",
    "\n",
    "# Escolhe o melhor dataset\n",
    "if df_mlg is not None and len(df_mlg) > len(df_csv) if df_csv is not None else 0:\n",
    "    df_log = df_mlg\n",
    "    print(f\"\\n‚úÖ Usando dados MLG ({len(df_log)} registros)\")\n",
    "elif df_csv is not None:\n",
    "    df_log = df_csv\n",
    "    print(f\"\\n‚úÖ Usando dados CSV ({len(df_log)} registros)\")\n",
    "else:\n",
    "    df_log = None\n",
    "    print(f\"\\n‚ùå Nenhum arquivo v√°lido encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb794c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtros de Qualidade para Melhor Precis√£o\n",
    "def apply_quality_filters(df, tps_threshold=2.0, lambda_range=(0.7, 1.3), min_pw=1.5):\n",
    "    \"\"\"Aplica filtros de qualidade conforme solicitado pelo usu√°rio\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    print(f\"üéØ Aplicando filtros de qualidade...\")\n",
    "    print(f\"üìä Registros iniciais: {initial_count}\")\n",
    "    \n",
    "    # 1. TPS est√°vel (mudan√ßa < threshold %/segundo)\n",
    "    tps_stable = abs(df['TPS_Rate']) <= tps_threshold\n",
    "    df = df[tps_stable]\n",
    "    print(f\"üéõÔ∏è Ap√≥s filtro TPS est√°vel (¬±{tps_threshold}%/s): {len(df)} registros ({len(df)/initial_count*100:.1f}%)\")\n",
    "    \n",
    "    # 2. Exclui momentos de fuel cut\n",
    "    no_fuel_cut = ~df['Is_FuelCut']\n",
    "    df = df[no_fuel_cut]\n",
    "    print(f\"‚õΩ Ap√≥s exclus√£o de fuel cut: {len(df)} registros ({len(df)/initial_count*100:.1f}%)\")\n",
    "    \n",
    "    # 3. Lambda v√°lido (range t√≠pico)\n",
    "    lambda_valid = (df['Lambda'] >= lambda_range[0]) & (df['Lambda'] <= lambda_range[1])\n",
    "    df = df[lambda_valid]\n",
    "    print(f\"üå°Ô∏è Ap√≥s filtro lambda ({lambda_range[0]}-{lambda_range[1]}): {len(df)} registros ({len(df)/initial_count*100:.1f}%)\")\n",
    "    \n",
    "    # 4. RPM e carga m√≠nimos para an√°lise confi√°vel\n",
    "    operating_range = (df['RPM'] >= 1000) & (df['FuelLoad'] >= 20)\n",
    "    df = df[operating_range]\n",
    "    print(f\"üîß Ap√≥s filtro de range operacional: {len(df)} registros ({len(df)/initial_count*100:.1f}%)\")\n",
    "    \n",
    "    if len(df) < 100:\n",
    "        print(f\"‚ö†Ô∏è Poucos dados ap√≥s filtros ({len(df)}). Considere relaxar crit√©rios.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Filtros aplicados: {len(df)} registros finais ({len(df)/initial_count*100:.1f}% dos dados originais)\")\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def analyze_tps_stability(df, window=50):\n",
    "    \"\"\"Analisa estabilidade do TPS para determinar threshold ideal\"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä An√°lise de estabilidade do TPS:\")\n",
    "    print(f\"TPS Rate - Min: {df['TPS_Rate'].min():.2f}, Max: {df['TPS_Rate'].max():.2f}\")\n",
    "    print(f\"TPS Rate - M√©dia: {df['TPS_Rate'].mean():.2f}, Std: {df['TPS_Rate'].std():.2f}\")\n",
    "    \n",
    "    # Percentis para entender distribui√ß√£o\n",
    "    percentiles = [50, 75, 90, 95, 99]\n",
    "    tps_rate_abs = abs(df['TPS_Rate'])\n",
    "    \n",
    "    print(f\"\\nPercentis de |TPS_Rate| (%/s):\")\n",
    "    for p in percentiles:\n",
    "        val = np.percentile(tps_rate_abs, p)\n",
    "        count = sum(tps_rate_abs <= val)\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  P{p}: {val:.2f} ({pct:.1f}% dos dados)\")\n",
    "    \n",
    "    # Visualiza√ß√£o da distribui√ß√£o\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Histograma de TPS Rate\n",
    "    ax1.hist(df['TPS_Rate'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(0, color='red', linestyle='--', label='Zero')\n",
    "    ax1.axvline(-2, color='orange', linestyle='--', label='¬±2%/s')\n",
    "    ax1.axvline(2, color='orange', linestyle='--')\n",
    "    ax1.set_xlabel('TPS Rate (%/s)')\n",
    "    ax1.set_ylabel('Frequ√™ncia')\n",
    "    ax1.set_title('Distribui√ß√£o da Taxa de Mudan√ßa do TPS')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # TPS ao longo do tempo (amostra)\n",
    "    sample_size = min(1000, len(df))\n",
    "    sample_idx = np.linspace(0, len(df)-1, sample_size, dtype=int)\n",
    "    \n",
    "    ax2.plot(df.iloc[sample_idx]['Time'], df.iloc[sample_idx]['THROTTLE_POS'], 'b-', alpha=0.7, label='TPS')\n",
    "    ax2.set_xlabel('Tempo (s)')\n",
    "    ax2.set_ylabel('TPS (%)')\n",
    "    ax2.set_title('TPS ao Longo do Tempo (amostra)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Aplica filtros se temos dados\n",
    "if df_log is not None:\n",
    "    analyze_tps_stability(df_log)\n",
    "    df_filtered = apply_quality_filters(df_log, tps_threshold=2.0)\n",
    "    \n",
    "    if df_filtered is not None:\n",
    "        print(f\"\\nüéØ Compara√ß√£o antes/depois dos filtros:\")\n",
    "        print(f\"Lambda - Original: Œº={df_log['Lambda'].mean():.3f}, œÉ={df_log['Lambda'].std():.3f}\")\n",
    "        print(f\"Lambda - Filtrado: Œº={df_filtered['Lambda'].mean():.3f}, œÉ={df_filtered['Lambda'].std():.3f}\")\n",
    "        print(f\"TPS Rate - Original: Œº={abs(df_log['TPS_Rate']).mean():.3f}\")\n",
    "        print(f\"TPS Rate - Filtrado: Œº={abs(df_filtered['TPS_Rate']).mean():.3f}\")\n",
    "else:\n",
    "    print(\"‚ùå Sem dados para aplicar filtros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d25c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para carregar tabela VE\n",
    "def load_ve_table(file_path):\n",
    "    \"\"\"Carrega tabela VE do arquivo XML\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Namespace\n",
    "        ns = {'table': 'http://www.EFIAnalytics.com/:table'}\n",
    "        \n",
    "        # Eixo X (RPM)\n",
    "        x_axis = root.find('.//table:xAxis', ns)\n",
    "        rpm_bins = [float(x) for x in x_axis.text.strip().split()]\n",
    "        \n",
    "        # Eixo Y (FuelLoad)\n",
    "        y_axis = root.find('.//table:yAxis', ns)\n",
    "        load_bins = [float(x) for x in y_axis.text.strip().split()]\n",
    "        \n",
    "        # Valores VE\n",
    "        z_values = root.find('.//table:zValues', ns)\n",
    "        ve_values = [float(x) for x in z_values.text.strip().split()]\n",
    "        \n",
    "        # Converte para matriz\n",
    "        ve_matrix = np.array(ve_values).reshape(len(load_bins), len(rpm_bins))\n",
    "        \n",
    "        print(f\"üóÇÔ∏è  Tabela VE carregada: {len(load_bins)}x{len(rpm_bins)} c√©lulas\")\n",
    "        print(f\"üìä RPM range: {min(rpm_bins):.0f} - {max(rpm_bins):.0f}\")\n",
    "        print(f\"üìä Load range: {min(load_bins):.0f} - {max(load_bins):.0f} kPa\")\n",
    "        print(f\"üìä VE range: {ve_matrix.min():.1f}% - {ve_matrix.max():.1f}%\")\n",
    "        \n",
    "        return rpm_bins, load_bins, ve_matrix\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar tabela VE: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Carrega tabela VE\n",
    "ve_file = '/home/robca/obd2/logs/ve.table'\n",
    "rpm_bins, load_bins, ve_table = load_ve_table(ve_file)\n",
    "\n",
    "# Visualiza a tabela VE original\n",
    "if ve_table is not None:\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Heatmap da tabela VE\n",
    "    plt.subplot(1, 2, 1)\n",
    "    im = plt.imshow(ve_table, aspect='auto', origin='lower', cmap='RdYlBu_r')\n",
    "    plt.colorbar(im, label='VE (%)')\n",
    "    plt.title('Tabela VE Original')\n",
    "    plt.xlabel('RPM Bins')\n",
    "    plt.ylabel('Load Bins (kPa)')\n",
    "    \n",
    "    # Configurar ticks\n",
    "    rpm_ticks = np.arange(0, len(rpm_bins), 2)\n",
    "    load_ticks = np.arange(0, len(load_bins), 2)\n",
    "    plt.xticks(rpm_ticks, [f'{rpm_bins[i]:.0f}' for i in rpm_ticks], rotation=45)\n",
    "    plt.yticks(load_ticks, [f'{load_bins[i]:.0f}' for i in load_ticks])\n",
    "    \n",
    "    # Histograma dos valores VE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(ve_table.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('VE (%)')\n",
    "    plt.ylabel('Frequ√™ncia')\n",
    "    plt.title('Distribui√ß√£o dos Valores VE')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd6dfcf",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5760a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise Explorat√≥ria dos Dados com Filtros de Qualidade\n",
    "\n",
    "# Usa dados filtrados se dispon√≠vel, sen√£o usa originais\n",
    "if 'df_filtered' in locals() and df_filtered is not None:\n",
    "    df_analysis = df_filtered\n",
    "    print(\"üéØ Usando dados com filtros de qualidade aplicados\")\n",
    "else:\n",
    "    df_analysis = df_log\n",
    "    print(\"üìä Usando dados originais (sem filtros)\")\n",
    "\n",
    "if df_analysis is None:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para an√°lise\")\n",
    "else:\n",
    "    print(f\"üìà Analisando {len(df_analysis)} registros\")\n",
    "    \n",
    "    # Estat√≠sticas descritivas\n",
    "    print(f\"\\nüìä Estat√≠sticas dos principais par√¢metros:\")\n",
    "    key_columns = ['RPM', 'Lambda', 'FuelLoad', 'THROTTLE_POS', 'TPS_Rate']\n",
    "    stats = df_analysis[key_columns].describe()\n",
    "    print(stats.round(3))\n",
    "    \n",
    "    # Visualiza√ß√µes melhoradas\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Distribui√ß√£o Lambda\n",
    "    axes[0,0].hist(df_analysis['Lambda'], bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    axes[0,0].axvline(1.0, color='red', linestyle='--', label='Stoich (Œª=1.0)')\n",
    "    axes[0,0].set_xlabel('Lambda')\n",
    "    axes[0,0].set_ylabel('Frequ√™ncia')\n",
    "    axes[0,0].set_title('Distribui√ß√£o Lambda')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Lambda vs RPM\n",
    "    scatter = axes[0,1].scatter(df_analysis['RPM'], df_analysis['Lambda'], \n",
    "                               c=df_analysis['FuelLoad'], cmap='viridis', alpha=0.6, s=1)\n",
    "    axes[0,1].axhline(1.0, color='red', linestyle='--', label='Stoich')\n",
    "    axes[0,1].set_xlabel('RPM')\n",
    "    axes[0,1].set_ylabel('Lambda')\n",
    "    axes[0,1].set_title('Lambda vs RPM (cor = FuelLoad)')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[0,1], label='FuelLoad')\n",
    "    \n",
    "    # 3. TPS Rate (novo)\n",
    "    axes[0,2].hist(abs(df_analysis['TPS_Rate']), bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[0,2].axvline(2.0, color='red', linestyle='--', label='Threshold (2%/s)')\n",
    "    axes[0,2].set_xlabel('|TPS Rate| (%/s)')\n",
    "    axes[0,2].set_ylabel('Frequ√™ncia')\n",
    "    axes[0,2].set_title('Distribui√ß√£o |TPS Rate|')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Lambda vs FuelLoad\n",
    "    scatter2 = axes[1,0].scatter(df_analysis['FuelLoad'], df_analysis['Lambda'], \n",
    "                                c=df_analysis['RPM'], cmap='plasma', alpha=0.6, s=1)\n",
    "    axes[1,0].axhline(1.0, color='red', linestyle='--', label='Stoich')\n",
    "    axes[1,0].set_xlabel('FuelLoad')\n",
    "    axes[1,0].set_ylabel('Lambda')\n",
    "    axes[1,0].set_title('Lambda vs FuelLoad (cor = RPM)')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter2, ax=axes[1,0], label='RPM')\n",
    "    \n",
    "    # 5. Timeline Lambda (amostra)\n",
    "    sample_size = min(2000, len(df_analysis))\n",
    "    sample_idx = np.linspace(0, len(df_analysis)-1, sample_size, dtype=int)\n",
    "    sample_data = df_analysis.iloc[sample_idx]\n",
    "    \n",
    "    axes[1,1].plot(sample_data['Time'], sample_data['Lambda'], 'b-', alpha=0.7, linewidth=0.5)\n",
    "    axes[1,1].axhline(1.0, color='red', linestyle='--', label='Stoich')\n",
    "    axes[1,1].set_xlabel('Tempo (s)')\n",
    "    axes[1,1].set_ylabel('Lambda')\n",
    "    axes[1,1].set_title('Lambda ao Longo do Tempo')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Correla√ß√£o com TPS\n",
    "    axes[1,2].scatter(df_analysis['THROTTLE_POS'], df_analysis['Lambda'], \n",
    "                     c=df_analysis['TPS_Rate'], cmap='RdYlBu', alpha=0.6, s=1)\n",
    "    axes[1,2].axhline(1.0, color='red', linestyle='--', label='Stoich')\n",
    "    axes[1,2].set_xlabel('TPS (%)')\n",
    "    axes[1,2].set_ylabel('Lambda')\n",
    "    axes[1,2].set_title('Lambda vs TPS (cor = TPS Rate)')\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # An√°lise de correla√ß√µes\n",
    "    print(f\"\\nüîó Matriz de correla√ß√µes:\")\n",
    "    corr_columns = ['RPM', 'Lambda', 'FuelLoad', 'THROTTLE_POS', 'TPS_Rate', 'INTAKE_TEMP']\n",
    "    correlations = df_analysis[corr_columns].corr()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlations, annot=True, cmap='RdBu_r', center=0, \n",
    "                square=True, fmt='.3f')\n",
    "    plt.title('Matriz de Correla√ß√µes')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Qualidade dos dados\n",
    "    print(f\"\\n‚úÖ Qualidade dos dados filtrados:\")\n",
    "    print(f\"‚Ä¢ Range Lambda: {df_analysis['Lambda'].min():.3f} - {df_analysis['Lambda'].max():.3f}\")\n",
    "    print(f\"‚Ä¢ Std Lambda: {df_analysis['Lambda'].std():.3f}\")\n",
    "    print(f\"‚Ä¢ % dados com TPS est√°vel: {(abs(df_analysis['TPS_Rate']) <= 2.0).mean()*100:.1f}%\")\n",
    "    print(f\"‚Ä¢ % dados sem fuel cut: {(~df_analysis['Is_FuelCut']).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cbb31b",
   "metadata": {},
   "source": [
    "## 3. Lambda Sensor Delay Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2d341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para estimar delay da sonda O2\n",
    "def estimate_lambda_sensor_delay(df):\n",
    "    \"\"\"Estima o delay da sonda lambda baseado em eventos de transiente\"\"\"\n",
    "    \n",
    "    print(\"üîç Analisando delay da sonda lambda...\")\n",
    "    \n",
    "    # 1. Detecta eventos de mudan√ßa r√°pida no TPS (acelera√ß√£o/desacelera√ß√£o)\n",
    "    tps_threshold = 30  # %/s\n",
    "    rapid_tps_events = df[abs(df['TPS_Rate']) > tps_threshold].copy()\n",
    "    \n",
    "    print(f\"üìà Eventos r√°pidos de TPS detectados: {len(rapid_tps_events)}\")\n",
    "    \n",
    "    # 2. Detecta cut-off de combust√≠vel (DFCO) - simulado por queda abrupta em lambda\n",
    "    # Identifica pontos onde lambda aumenta rapidamente (corte de combust√≠vel)\n",
    "    lambda_spike_threshold = 0.1  # lambda/s\n",
    "    fuel_cut_events = df[df['Lambda_Rate'] > lambda_spike_threshold].copy()\n",
    "    \n",
    "    print(f\"‚õΩ Eventos de corte de combust√≠vel detectados: {len(fuel_cut_events)}\")\n",
    "    \n",
    "    # 3. Analisa correla√ß√£o cruzada para estimar delay\n",
    "    delays = []\n",
    "    \n",
    "    # Para cada evento de TPS r√°pido\n",
    "    for idx in rapid_tps_events.index[:20]:  # Analisa at√© 20 eventos\n",
    "        if idx + 100 < len(df):  # Garante janela de an√°lise\n",
    "            \n",
    "            # Janela de an√°lise (10 segundos)\n",
    "            window_start = max(0, idx - 50)\n",
    "            window_end = min(len(df), idx + 100)\n",
    "            window = df.iloc[window_start:window_end].copy()\n",
    "            \n",
    "            # Sinais para correla√ß√£o\n",
    "            tps_signal = window['TPS_Rate'].values\n",
    "            lambda_signal = window['Lambda_Rate'].values\n",
    "            \n",
    "            # Correla√ß√£o cruzada\n",
    "            correlation = np.correlate(lambda_signal, tps_signal, mode='full')\n",
    "            \n",
    "            # Encontra o lag com maior correla√ß√£o\n",
    "            lag_samples = np.argmax(np.abs(correlation)) - len(tps_signal) + 1\n",
    "            \n",
    "            # Converte para tempo (segundos)\n",
    "            lag_time = lag_samples * 0.1\n",
    "            \n",
    "            # Filtra delays plaus√≠veis (0.1 a 2.0 segundos)\n",
    "            if 0.1 <= lag_time <= 2.0:\n",
    "                delays.append(lag_time)\n",
    "    \n",
    "    # Estat√≠sticas dos delays\n",
    "    if delays:\n",
    "        estimated_delay = np.median(delays)\n",
    "        delay_std = np.std(delays)\n",
    "        delay_samples = int(estimated_delay / 0.1)\n",
    "        \n",
    "        print(f\"‚è±Ô∏è  Delay estimado: {estimated_delay:.2f}s ¬± {delay_std:.2f}s\")\n",
    "        print(f\"üìä Baseado em {len(delays)} eventos v√°lidos\")\n",
    "        print(f\"üî¢ Delay em amostras: {delay_samples}\")\n",
    "        \n",
    "        # Visualiza an√°lise de delay\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Histograma dos delays\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.hist(delays, bins=20, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(estimated_delay, color='red', linestyle='--', label=f'Mediana: {estimated_delay:.2f}s')\n",
    "        plt.xlabel('Delay (segundos)')\n",
    "        plt.ylabel('Frequ√™ncia')\n",
    "        plt.title('Distribui√ß√£o dos Delays Estimados')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Exemplo de evento\n",
    "        if len(rapid_tps_events) > 0:\n",
    "            example_idx = rapid_tps_events.index[0]\n",
    "            window_start = max(0, example_idx - 50)\n",
    "            window_end = min(len(df), example_idx + 100)\n",
    "            example_window = df.iloc[window_start:window_end].copy()\n",
    "            \n",
    "            plt.subplot(2, 3, 2)\n",
    "            plt.plot(example_window['Time'], example_window['THROTTLE_POS'], label='TPS (%)', linewidth=2)\n",
    "            plt.axvline(df.iloc[example_idx]['Time'], color='red', linestyle='--', alpha=0.7, label='Evento TPS')\n",
    "            plt.xlabel('Tempo (s)')\n",
    "            plt.ylabel('TPS (%)')\\n            plt.title('Exemplo: Evento de TPS')\\n            plt.legend()\\n            plt.grid(True, alpha=0.3)\\n            \\n            plt.subplot(2, 3, 3)\\n            plt.plot(example_window['Time'], example_window['Lambda'], label='Lambda', linewidth=2, color='orange')\\n            plt.axvline(df.iloc[example_idx]['Time'], color='red', linestyle='--', alpha=0.7, label='Evento TPS')\\n            plt.axvline(df.iloc[example_idx]['Time'] + estimated_delay, color='green', \\n                       linestyle='--', alpha=0.7, label=f'Lambda Response (+{estimated_delay:.2f}s)')\\n            plt.xlabel('Tempo (s)')\\n            plt.ylabel('Lambda')\\n            plt.title('Resposta do Lambda')\\n            plt.legend()\\n            plt.grid(True, alpha=0.3)\\n        \\n        # An√°lise de eventos de TPS\\n        plt.subplot(2, 3, 4)\\n        plt.scatter(df['Time'], df['TPS_Rate'], alpha=0.3, s=1, label='TPS Rate')\\n        plt.scatter(rapid_tps_events['Time'], rapid_tps_events['TPS_Rate'], \\n                   color='red', s=20, label=f'Eventos R√°pidos (>{tps_threshold}%/s)')\\n        plt.xlabel('Tempo (s)')\\n        plt.ylabel('TPS Rate (%/s)')\\n        plt.title('Detec√ß√£o de Eventos de TPS')\\n        plt.legend()\\n        plt.grid(True, alpha=0.3)\\n        \\n        # An√°lise de eventos de lambda\\n        plt.subplot(2, 3, 5)\\n        plt.scatter(df['Time'], df['Lambda_Rate'], alpha=0.3, s=1, label='Lambda Rate')\\n        plt.scatter(fuel_cut_events['Time'], fuel_cut_events['Lambda_Rate'], \\n                   color='orange', s=20, label=f'Fuel Cut (>{lambda_spike_threshold}/s)')\\n        plt.xlabel('Tempo (s)')\\n        plt.ylabel('Lambda Rate (/s)')\\n        plt.title('Detec√ß√£o de Eventos de Lambda')\\n        plt.legend()\\n        plt.grid(True, alpha=0.3)\\n        \\n        # Correla√ß√£o TPS vs Lambda com delay\\n        plt.subplot(2, 3, 6)\\n        # Aplica delay ao lambda\\n        lambda_delayed = df['Lambda'].shift(delay_samples)\\n        valid_mask = ~(df['TPS_Rate'].isna() | lambda_delayed.isna())\\n        plt.scatter(df[valid_mask]['TPS_Rate'], lambda_delayed[valid_mask], alpha=0.5, s=1)\\n        plt.xlabel('TPS Rate (%/s)')\\n        plt.ylabel(f'Lambda (delayed +{estimated_delay:.2f}s)')\\n        plt.title('Correla√ß√£o TPS vs Lambda (com delay)')\\n        plt.grid(True, alpha=0.3)\\n        \\n        plt.tight_layout()\\n        plt.show()\\n        \\n        return estimated_delay, delay_samples, delays\\n        \\n    else:\\n        print(\\\"‚ùå N√£o foi poss√≠vel estimar delay da sonda\\\")\\n        return 0.0, 0, []\\n\\n# Estima delay da sonda\\nif df_processed is not None:\\n    estimated_delay, delay_samples, delay_list = estimate_lambda_sensor_delay(df_processed)\\nelse:\\n    estimated_delay, delay_samples = 0.0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7161cf2",
   "metadata": {},
   "source": [
    "## 4. VE Table Current vs Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbbf0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√µes para an√°lise VE vs Target\n",
    "def interpolate_ve_current(rpm, fuel_load, rpm_bins, load_bins, ve_table):\n",
    "    \"\"\"Interpola VE atual da tabela para RPM e Load espec√≠ficos\"\"\"\n",
    "    if ve_table is None:\n",
    "        return 50.0\n",
    "    \n",
    "    # Interpola√ß√£o bilinear\n",
    "    interp_func = interpolate.interp2d(rpm_bins, load_bins, ve_table, kind='linear')\n",
    "    \n",
    "    # Limita valores aos ranges da tabela\n",
    "    rpm_clipped = np.clip(rpm, min(rpm_bins), max(rpm_bins))\n",
    "    load_clipped = np.clip(fuel_load, min(load_bins), max(load_bins))\n",
    "    \n",
    "    return float(interp_func(rpm_clipped, load_clipped)[0])\n",
    "\n",
    "def calculate_lambda_target(rpm, fuel_load):\n",
    "    \"\"\"Calcula lambda target baseado em condi√ß√µes de opera√ß√£o\"\"\"\n",
    "    # Estrat√©gia de lambda target:\\n    # - Baixa carga (cruise): Œª ‚âà 1.0 (estequiom√©trico)\\n    # - Carga m√©dia (acelera√ß√£o): Œª ‚âà 0.95\\n    # - Alta carga (WOT): Œª ‚âà 0.85-0.87 (prote√ß√£o)\\n    \\n    if fuel_load < 25:\\n        return 1.0  # Cruise - economia\\n    elif fuel_load < 60:\\n        return 0.95  # Acelera√ß√£o moderada\\n    elif fuel_load < 80:\\n        return 0.88  # Acelera√ß√£o forte\\n    else:\\n        return 0.85  # WOT - prote√ß√£o\\n\\n# An√°lise dos dados com VE e targets\\ndef analyze_ve_vs_target(df, rpm_bins, load_bins, ve_table, delay_samples=0):\\n    \\\"\\\"\\\"Analisa VE atual vs lambda target e calcula corre√ß√µes necess√°rias\\\"\\\"\\\"\\n    \\n    print(\\\"üîç Analisando VE atual vs Lambda target...\\\")\\n    \\n    # Cria c√≥pia dos dados\\n    df_analysis = df.copy()\\n    \\n    # Interpola VE atual para cada ponto\\n    df_analysis['VE_Current'] = df_analysis.apply(\\n        lambda row: interpolate_ve_current(row['RPM'], row['FuelLoad'], rpm_bins, load_bins, ve_table),\\n        axis=1\\n    )\\n    \\n    # Calcula lambda target para cada ponto\\n    df_analysis['Lambda_Target'] = df_analysis.apply(\\n        lambda row: calculate_lambda_target(row['RPM'], row['FuelLoad']),\\n        axis=1\\n    )\\n    \\n    # Aplica delay ao lambda se estimado\\n    if delay_samples > 0:\\n        df_analysis['Lambda_Delayed'] = df_analysis['Lambda'].shift(-delay_samples)\\n        # Remove pontos sem lambda delayed\\n        df_analysis = df_analysis[:-delay_samples] if delay_samples > 0 else df_analysis\\n    else:\\n        df_analysis['Lambda_Delayed'] = df_analysis['Lambda']\\n    \\n    # Remove NaN\\n    df_analysis = df_analysis.dropna()\\n    \\n    # Calcula erros e corre√ß√µes\\n    df_analysis['Lambda_Error'] = df_analysis['Lambda_Delayed'] - df_analysis['Lambda_Target']\\n    df_analysis['Lambda_Error_Percent'] = (df_analysis['Lambda_Error'] / df_analysis['Lambda_Target']) * 100\\n    \\n    # Calcula fator de corre√ß√£o VE\\n    # Se lambda √© maior que target (mistura pobre), precisa mais combust√≠vel (VE maior)\\n    df_analysis['VE_Correction_Factor'] = df_analysis['Lambda_Target'] / df_analysis['Lambda_Delayed']\\n    df_analysis['VE_Corrected'] = df_analysis['VE_Current'] * df_analysis['VE_Correction_Factor']\\n    \\n    # Limita corre√ß√µes extremas (¬±20%)\\n    df_analysis['VE_Corrected'] = np.clip(\\n        df_analysis['VE_Corrected'],\\n        df_analysis['VE_Current'] * 0.8,\\n        df_analysis['VE_Current'] * 1.2\\n    )\\n    \\n    df_analysis['VE_Change_Percent'] = ((df_analysis['VE_Corrected'] - df_analysis['VE_Current']) / \\n                                       df_analysis['VE_Current']) * 100\\n    \\n    print(f\\\"üìä Dados para an√°lise: {len(df_analysis)} pontos\\\")\\n    print(f\\\"‚öñÔ∏è  Erro m√©dio de lambda: {df_analysis['Lambda_Error'].mean():.3f}\\\")\\n    print(f\\\"üìà Corre√ß√£o m√©dia de VE: {df_analysis['VE_Change_Percent'].mean():.1f}%\\\")\\n    \\n    return df_analysis\\n\\n# Executa an√°lise\\nif df_processed is not None and ve_table is not None:\\n    df_ve_analysis = analyze_ve_vs_target(df_processed, rpm_bins, load_bins, ve_table, delay_samples)\\n    \\n    # Visualiza√ß√µes da an√°lise\\n    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\\n    \\n    # 1. Lambda atual vs target\\n    axes[0,0].scatter(df_ve_analysis['Lambda_Target'], df_ve_analysis['Lambda_Delayed'], alpha=0.5, s=1)\\n    axes[0,0].plot([0.8, 1.1], [0.8, 1.1], 'r--', linewidth=2, label='Ideal (1:1)')\\n    axes[0,0].set_xlabel('Lambda Target')\\n    axes[0,0].set_ylabel('Lambda Medido (delayed)')\\n    axes[0,0].set_title('Lambda Medido vs Target')\\n    axes[0,0].legend()\\n    axes[0,0].grid(True, alpha=0.3)\\n    \\n    # 2. Erro de lambda por RPM\\n    axes[0,1].scatter(df_ve_analysis['RPM'], df_ve_analysis['Lambda_Error'], alpha=0.5, s=1, c=df_ve_analysis['FuelLoad'], cmap='viridis')\\n    axes[0,1].axhline(y=0, color='red', linestyle='--', alpha=0.7)\\n    axes[0,1].set_xlabel('RPM')\\n    axes[0,1].set_ylabel('Erro Lambda')\\n    axes[0,1].set_title('Erro Lambda por RPM (cor = FuelLoad)')\\n    axes[0,1].grid(True, alpha=0.3)\\n    \\n    # 3. Erro de lambda por carga\\n    axes[0,2].scatter(df_ve_analysis['FuelLoad'], df_ve_analysis['Lambda_Error'], alpha=0.5, s=1, c=df_ve_analysis['RPM'], cmap='plasma')\\n    axes[0,2].axhline(y=0, color='red', linestyle='--', alpha=0.7)\\n    axes[0,2].set_xlabel('Fuel Load (kPa)')\\n    axes[0,2].set_ylabel('Erro Lambda')\\n    axes[0,2].set_title('Erro Lambda por Carga (cor = RPM)')\\n    axes[0,2].grid(True, alpha=0.3)\\n    \\n    # 4. VE atual vs corrigido\\n    axes[1,0].scatter(df_ve_analysis['VE_Current'], df_ve_analysis['VE_Corrected'], alpha=0.5, s=1)\\n    min_ve = min(df_ve_analysis['VE_Current'].min(), df_ve_analysis['VE_Corrected'].min())\\n    max_ve = max(df_ve_analysis['VE_Current'].max(), df_ve_analysis['VE_Corrected'].max())\\n    axes[1,0].plot([min_ve, max_ve], [min_ve, max_ve], 'r--', linewidth=2, label='Sem mudan√ßa')\\n    axes[1,0].set_xlabel('VE Atual (%)')\\n    axes[1,0].set_ylabel('VE Corrigido (%)')\\n    axes[1,0].set_title('VE Atual vs Corrigido')\\n    axes[1,0].legend()\\n    axes[1,0].grid(True, alpha=0.3)\\n    \\n    # 5. Corre√ß√£o VE por RPM\\n    axes[1,1].scatter(df_ve_analysis['RPM'], df_ve_analysis['VE_Change_Percent'], alpha=0.5, s=1, c=df_ve_analysis['FuelLoad'], cmap='viridis')\\n    axes[1,1].axhline(y=0, color='red', linestyle='--', alpha=0.7)\\n    axes[1,1].set_xlabel('RPM')\\n    axes[1,1].set_ylabel('Mudan√ßa VE (%)')\\n    axes[1,1].set_title('Corre√ß√£o VE por RPM')\\n    axes[1,1].grid(True, alpha=0.3)\\n    \\n    # 6. Corre√ß√£o VE por carga\\n    axes[1,2].scatter(df_ve_analysis['FuelLoad'], df_ve_analysis['VE_Change_Percent'], alpha=0.5, s=1, c=df_ve_analysis['RPM'], cmap='plasma')\\n    axes[1,2].axhline(y=0, color='red', linestyle='--', alpha=0.7)\\n    axes[1,2].set_xlabel('Fuel Load (kPa)')\\n    axes[1,2].set_ylabel('Mudan√ßa VE (%)')\\n    axes[1,2].set_title('Corre√ß√£o VE por Carga')\\n    axes[1,2].grid(True, alpha=0.3)\\n    \\n    # 7. Histograma de erros lambda\\n    axes[2,0].hist(df_ve_analysis['Lambda_Error'], bins=50, alpha=0.7, edgecolor='black')\\n    axes[2,0].axvline(0, color='red', linestyle='--', alpha=0.7)\\n    axes[2,0].set_xlabel('Erro Lambda')\\n    axes[2,0].set_ylabel('Frequ√™ncia')\\n    axes[2,0].set_title('Distribui√ß√£o do Erro Lambda')\\n    axes[2,0].grid(True, alpha=0.3)\\n    \\n    # 8. Histograma de corre√ß√µes VE\\n    axes[2,1].hist(df_ve_analysis['VE_Change_Percent'], bins=50, alpha=0.7, edgecolor='black')\\n    axes[2,1].axvline(0, color='red', linestyle='--', alpha=0.7)\\n    axes[2,1].set_xlabel('Mudan√ßa VE (%)')\\n    axes[2,1].set_ylabel('Frequ√™ncia')\\n    axes[2,1].set_title('Distribui√ß√£o das Corre√ß√µes VE')\\n    axes[2,1].grid(True, alpha=0.3)\\n    \\n    # 9. Correla√ß√£o entre erro lambda e corre√ß√£o VE\\n    axes[2,2].scatter(df_ve_analysis['Lambda_Error'], df_ve_analysis['VE_Change_Percent'], alpha=0.5, s=1)\\n    axes[2,2].set_xlabel('Erro Lambda')\\n    axes[2,2].set_ylabel('Mudan√ßa VE (%)')\\n    axes[2,2].set_title('Correla√ß√£o: Erro Lambda vs Corre√ß√£o VE')\\n    axes[2,2].grid(True, alpha=0.3)\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    # Estat√≠sticas por zona de opera√ß√£o\\n    print(\\\"\\\\nüìä Estat√≠sticas por zona de opera√ß√£o:\\\")\\n    \\n    # Divide em zonas baseadas em carga\\n    zones = [\\n        (df_ve_analysis['FuelLoad'] < 30, 'Baixa Carga'),\\n        ((df_ve_analysis['FuelLoad'] >= 30) & (df_ve_analysis['FuelLoad'] < 70), 'Carga M√©dia'),\\n        (df_ve_analysis['FuelLoad'] >= 70, 'Alta Carga')\\n    ]\\n    \\n    for mask, zone_name in zones:\\n        zone_data = df_ve_analysis[mask]\\n        if len(zone_data) > 0:\\n            print(f\\\"\\\\n{zone_name}:\\\")\\n            print(f\\\"  Pontos: {len(zone_data)}\\\")\\n            print(f\\\"  Erro lambda m√©dio: {zone_data['Lambda_Error'].mean():.3f} ¬± {zone_data['Lambda_Error'].std():.3f}\\\")\\n            print(f\\\"  Corre√ß√£o VE m√©dia: {zone_data['VE_Change_Percent'].mean():.1f}% ¬± {zone_data['VE_Change_Percent'].std():.1f}%\\\")\\n            print(f\\\"  Range RPM: {zone_data['RPM'].min():.0f} - {zone_data['RPM'].max():.0f}\\\")\\nelse:\\n    print(\\\"‚ùå Dados n√£o dispon√≠veis para an√°lise VE\\\")\\n    df_ve_analysis = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d4407",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering for Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering para modelo de predi√ß√£o\\ndef create_features_for_ml(df_analysis):\\n    \\\"\\\"\\\"Cria features para treinamento do modelo de ML\\\"\\\"\\\"\\n    \\n    print(\\\"üîß Criando features para modelo de ML...\\\")\\n    \\n    df_features = df_analysis.copy()\\n    \\n    # 1. Features b√°sicas j√° existentes\\n    basic_features = ['RPM', 'FuelLoad', 'VE_Current', 'Lambda_Delayed', 'Lambda_Target',\\n                     'COOLANT_TEMP', 'INTAKE_TEMP', 'TIMING_ADVANCE']\\n    \\n    # 2. Features de intera√ß√£o\\n    df_features['RPM_Load_Interaction'] = df_features['RPM'] * df_features['FuelLoad'] / 1000\\n    df_features['VE_Lambda_Interaction'] = df_features['VE_Current'] * df_features['Lambda_Delayed']\\n    \\n    # 3. Features categ√≥ricas baseadas em ranges\\n    # Range de RPM\\n    df_features['RPM_Range'] = pd.cut(df_features['RPM'], \\n                                     bins=[0, 1500, 3000, 5000, 8000], \\n                                     labels=['Idle', 'Low', 'Mid', 'High'])\\n    \\n    # Range de Carga\\n    df_features['Load_Range'] = pd.cut(df_features['FuelLoad'], \\n                                      bins=[0, 30, 70, 100, 150], \\n                                      labels=['Light', 'Moderate', 'Heavy', 'WOT'])\\n    \\n    # 4. Features temporais (tend√™ncias)\\n    # Suaviza√ß√£o para reduzir ru√≠do\\n    window = 5\\n    df_features['RPM_Trend'] = df_features['RPM'].rolling(window).mean().fillna(df_features['RPM'])\\n    df_features['Load_Trend'] = df_features['FuelLoad'].rolling(window).mean().fillna(df_features['FuelLoad'])\\n    df_features['Lambda_Trend'] = df_features['Lambda_Delayed'].rolling(window).mean().fillna(df_features['Lambda_Delayed'])\\n    \\n    # 5. Features de efici√™ncia\\n    # Efici√™ncia atual (qu√£o longe do target)\\n    df_features['Lambda_Efficiency'] = 1 - abs(df_features['Lambda_Error']) / df_features['Lambda_Target']\\n    \\n    # 6. Features f√≠sicas do motor\\n    # Velocidade m√©dia do pist√£o (aproxima√ß√£o)\\n    df_features['Mean_Piston_Speed'] = df_features['RPM'] * 0.1  # simplificado\\n    \\n    # Densidade do ar (aproxima√ß√£o baseada em temp)\\n    df_features['Air_Density_Factor'] = 1 / (df_features['INTAKE_TEMP'] + 273.15)  # Lei dos gases ideais\\n    \\n    # 7. Features de opera√ß√£o do motor\\n    # Carga espec√≠fica (load normalizado por RPM)\\n    df_features['Specific_Load'] = df_features['FuelLoad'] / (df_features['RPM'] / 1000)\\n    \\n    # 8. One-hot encoding para features categ√≥ricas\\n    df_encoded = pd.get_dummies(df_features, columns=['RPM_Range', 'Load_Range'], prefix=['RPM', 'Load'])\\n    \\n    # 9. Features de posi√ß√£o na tabela (normalized grid position)\\n    if 'rpm_bins' in globals() and 'load_bins' in globals():\\n        # Posi√ß√£o normalizada na grid\\n        df_encoded['RPM_Grid_Position'] = np.interp(df_encoded['RPM'], rpm_bins, np.linspace(0, 1, len(rpm_bins)))\\n        df_encoded['Load_Grid_Position'] = np.interp(df_encoded['FuelLoad'], load_bins, np.linspace(0, 1, len(load_bins)))\\n    \\n    # Lista final de features para o modelo\\n    ml_features = [\\n        # Features b√°sicas\\n        'RPM', 'FuelLoad', 'VE_Current', 'Lambda_Delayed', 'Lambda_Target',\\n        'COOLANT_TEMP', 'INTAKE_TEMP', 'TIMING_ADVANCE',\\n        \\n        # Features de intera√ß√£o\\n        'RPM_Load_Interaction', 'VE_Lambda_Interaction',\\n        \\n        # Features de tend√™ncia\\n        'RPM_Trend', 'Load_Trend', 'Lambda_Trend',\\n        \\n        # Features de efici√™ncia\\n        'Lambda_Efficiency',\\n        \\n        # Features f√≠sicas\\n        'Mean_Piston_Speed', 'Air_Density_Factor', 'Specific_Load'\\n    ]\\n    \\n    # Adiciona features categ√≥ricas one-hot encoded\\n    categorical_features = [col for col in df_encoded.columns if col.startswith(('RPM_', 'Load_'))]\\n    ml_features.extend(categorical_features)\\n    \\n    # Adiciona features de posi√ß√£o se dispon√≠veis\\n    if 'RPM_Grid_Position' in df_encoded.columns:\\n        ml_features.extend(['RPM_Grid_Position', 'Load_Grid_Position'])\\n    \\n    # Remove features que podem ter NaN\\n    df_encoded = df_encoded.dropna(subset=ml_features)\\n    \\n    print(f\\\"‚úÖ Features criadas: {len(ml_features)}\\\")\\n    print(f\\\"üìä Dados para ML: {len(df_encoded)} amostras\\\")\\n    \\n    # Estat√≠sticas das features\\n    print(f\\\"\\\\nüìà Principais features criadas:\\\")\\n    for feature in ml_features[:10]:  # Mostra primeiras 10\\n        if feature in df_encoded.columns:\\n            print(f\\\"  {feature}: {df_encoded[feature].mean():.3f} ¬± {df_encoded[feature].std():.3f}\\\")\\n    \\n    return df_encoded, ml_features\\n\\n# Visualiza√ß√£o das features\\ndef visualize_features(df_features, ml_features):\\n    \\\"\\\"\\\"Visualiza distribui√ß√µes e correla√ß√µes das features\\\"\\\"\\\"\\n    \\n    # Seleciona features num√©ricas principais\\n    numeric_features = [f for f in ml_features if f in df_features.columns and \\n                       df_features[f].dtype in ['float64', 'int64']][:12]\\n    \\n    if len(numeric_features) > 0:\\n        # Matriz de correla√ß√£o\\n        plt.figure(figsize=(15, 12))\\n        \\n        # Correla√ß√£o entre features\\n        plt.subplot(2, 2, 1)\\n        corr_matrix = df_features[numeric_features].corr()\\n        sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, square=True)\\n        plt.title('Matriz de Correla√ß√£o das Features')\\n        \\n        # Distribui√ß√µes das principais features\\n        plt.subplot(2, 2, 2)\\n        df_features[['RPM', 'FuelLoad', 'VE_Current']].hist(bins=30, alpha=0.7)\\n        plt.title('Distribui√ß√µes das Features Principais')\\n        \\n        # Correla√ß√£o com target (VE_Corrected)\\n        plt.subplot(2, 2, 3)\\n        if 'VE_Corrected' in df_features.columns:\\n            correlations = []\\n            for feature in numeric_features:\\n                corr = df_features[feature].corr(df_features['VE_Corrected'])\\n                correlations.append(corr)\\n            \\n            feature_names = [f.replace('_', '\\\\n') for f in numeric_features]\\n            plt.barh(range(len(correlations)), correlations)\\n            plt.yticks(range(len(correlations)), feature_names)\\n            plt.xlabel('Correla√ß√£o com VE_Corrected')\\n            plt.title('Import√¢ncia das Features')\\n            plt.grid(True, alpha=0.3)\\n        \\n        # Feature importance plot (simplified)\\n        plt.subplot(2, 2, 4)\\n        # Vari√¢ncia das features (normalizada)\\n        variances = []\\n        for feature in numeric_features[:8]:  # Top 8\\n            var = df_features[feature].var()\\n            variances.append(var)\\n        \\n        # Normaliza vari√¢ncias\\n        variances = np.array(variances)\\n        variances = variances / variances.max()\\n        \\n        feature_names = [f.replace('_', '\\\\n') for f in numeric_features[:8]]\\n        plt.barh(range(len(variances)), variances)\\n        plt.yticks(range(len(variances)), feature_names)\\n        plt.xlabel('Vari√¢ncia Normalizada')\\n        plt.title('Variabilidade das Features')\\n        plt.grid(True, alpha=0.3)\\n        \\n        plt.tight_layout()\\n        plt.show()\\n\\n# Executa feature engineering\\nif 'df_ve_analysis' in locals() and df_ve_analysis is not None:\\n    df_ml_ready, feature_list = create_features_for_ml(df_ve_analysis)\\n    \\n    # Visualiza features\\n    visualize_features(df_ml_ready, feature_list)\\n    \\n    print(f\\\"\\\\nüéØ Target variable: VE_Corrected\\\")\\n    print(f\\\"üìä Target range: {df_ml_ready['VE_Corrected'].min():.1f}% - {df_ml_ready['VE_Corrected'].max():.1f}%\\\")\\n    print(f\\\"üìà Target mean: {df_ml_ready['VE_Corrected'].mean():.1f}% ¬± {df_ml_ready['VE_Corrected'].std():.1f}%\\\")\\nelse:\\n    print(\\\"‚ùå Dados de an√°lise VE n√£o dispon√≠veis para feature engineering\\\")\\n    df_ml_ready, feature_list = None, []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42d195",
   "metadata": {},
   "source": [
    "## 6. Machine Learning Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca8cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento de modelos de Machine Learning\n",
    "def train_ve_prediction_models(df_ml, features, target='VE_Corrected'):\n",
    "    \\\"\\\"\\\"Treina m√∫ltiplos modelos para predi√ß√£o de VE otimizado\\\"\\\"\\\"\n",
    "    \n",
    "    print(\\\"ü§ñ Treinando modelos de ML para predi√ß√£o de VE...\\\")\\n    \\n    # Prepara dados\\n    X = df_ml[features].copy()\\n    y = df_ml[target].copy()\\n    \\n    # Remove linhas com NaN\\n    mask = ~(X.isnull().any(axis=1) | y.isnull())\\n    X = X[mask]\\n    y = y[mask]\\n    \\n    print(f\\\"üìä Dados de treinamento: {len(X)} amostras, {len(features)} features\\\")\\n    \\n    # Normaliza√ß√£o das features\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n    X_scaled_df = pd.DataFrame(X_scaled, columns=features, index=X.index)\\n    \\n    # Split train/validation/test\\n    X_temp, X_test, y_temp, y_test = train_test_split(X_scaled_df, y, test_size=0.2, random_state=42)\\n    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\\n    \\n    print(f\\\"üìä Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\\\")\\n    \\n    # Modelos para testar\\n    models = {\\n        'Random Forest': RandomForestRegressor(\\n            n_estimators=200, \\n            max_depth=15, \\n            min_samples_split=5,\\n            min_samples_leaf=2,\\n            random_state=42,\\n            n_jobs=-1\\n        ),\\n        'Gradient Boosting': GradientBoostingRegressor(\\n            n_estimators=200,\\n            max_depth=8,\\n            learning_rate=0.1,\\n            subsample=0.8,\\n            random_state=42\\n        ),\\n        'Linear Regression': LinearRegression()\\n    }\\n    \\n    # Treina e avalia modelos\\n    model_results = {}\\n    best_model = None\\n    best_score = float('inf')\\n    \\n    for name, model in models.items():\\n        print(f\\\"\\\\nüîÑ Treinando {name}...\\\")\\n        \\n        # Treina modelo\\n        model.fit(X_train, y_train)\\n        \\n        # Predi√ß√µes\\n        y_train_pred = model.predict(X_train)\\n        y_val_pred = model.predict(X_val)\\n        y_test_pred = model.predict(X_test)\\n        \\n        # M√©tricas\\n        results = {\\n            'model': model,\\n            'train_mse': mean_squared_error(y_train, y_train_pred),\\n            'val_mse': mean_squared_error(y_val, y_val_pred),\\n            'test_mse': mean_squared_error(y_test, y_test_pred),\\n            'train_r2': r2_score(y_train, y_train_pred),\\n            'val_r2': r2_score(y_val, y_val_pred),\\n            'test_r2': r2_score(y_test, y_test_pred),\\n            'train_mae': mean_absolute_error(y_train, y_train_pred),\\n            'val_mae': mean_absolute_error(y_val, y_val_pred),\\n            'test_mae': mean_absolute_error(y_test, y_test_pred),\\n            'predictions': {\\n                'train': y_train_pred,\\n                'val': y_val_pred,\\n                'test': y_test_pred\\n            }\\n        }\\n        \\n        model_results[name] = results\\n        \\n        # Atualiza melhor modelo baseado em validation score\\n        if results['val_mse'] < best_score:\\n            best_score = results['val_mse']\\n            best_model = (name, model)\\n        \\n        # Imprime resultados\\n        print(f\\\"  Train - MSE: {results['train_mse']:.3f}, R¬≤: {results['train_r2']:.3f}, MAE: {results['train_mae']:.3f}\\\")\\n        print(f\\\"  Val   - MSE: {results['val_mse']:.3f}, R¬≤: {results['val_r2']:.3f}, MAE: {results['val_mae']:.3f}\\\")\\n        print(f\\\"  Test  - MSE: {results['test_mse']:.3f}, R¬≤: {results['test_r2']:.3f}, MAE: {results['test_mae']:.3f}\\\")\\n    \\n    print(f\\\"\\\\nüèÜ Melhor modelo: {best_model[0]} (Val MSE: {best_score:.3f})\\\")\\n    \\n    # Visualiza compara√ß√£o dos modelos\\n    plt.figure(figsize=(18, 12))\\n    \\n    # M√©tricas comparativas\\n    plt.subplot(2, 3, 1)\\n    model_names = list(model_results.keys())\\n    val_r2_scores = [model_results[name]['val_r2'] for name in model_names]\\n    colors = ['skyblue', 'lightgreen', 'lightcoral']\\n    bars = plt.bar(model_names, val_r2_scores, color=colors)\\n    plt.ylabel('R¬≤ Score (Validation)')\\n    plt.title('Compara√ß√£o R¬≤ dos Modelos')\\n    plt.xticks(rotation=45)\\n    # Adiciona valores nas barras\\n    for bar, score in zip(bars, val_r2_scores):\\n        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \\n                f'{score:.3f}', ha='center', va='bottom')\\n    plt.grid(True, alpha=0.3)\\n    \\n    # MSE comparativo\\n    plt.subplot(2, 3, 2)\\n    val_mse_scores = [model_results[name]['val_mse'] for name in model_names]\\n    bars = plt.bar(model_names, val_mse_scores, color=colors)\\n    plt.ylabel('MSE (Validation)')\\n    plt.title('Compara√ß√£o MSE dos Modelos')\\n    plt.xticks(rotation=45)\\n    for bar, score in zip(bars, val_mse_scores):\\n        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \\n                f'{score:.2f}', ha='center', va='bottom')\\n    plt.grid(True, alpha=0.3)\\n    \\n    # Predi√ß√µes vs Real (melhor modelo)\\n    best_name = best_model[0]\\n    best_results = model_results[best_name]\\n    \\n    plt.subplot(2, 3, 3)\\n    plt.scatter(y_test, best_results['predictions']['test'], alpha=0.6, s=10)\\n    min_val = min(y_test.min(), best_results['predictions']['test'].min())\\n    max_val = max(y_test.max(), best_results['predictions']['test'].max())\\n    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfeito')\\n    plt.xlabel('VE Real (%)')\\n    plt.ylabel('VE Predito (%)')\\n    plt.title(f'Predi√ß√µes vs Real - {best_name}')\\n    plt.legend()\\n    plt.grid(True, alpha=0.3)\\n    \\n    # Res√≠duos\\n    plt.subplot(2, 3, 4)\\n    residuals = y_test - best_results['predictions']['test']\\n    plt.scatter(best_results['predictions']['test'], residuals, alpha=0.6, s=10)\\n    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)\\n    plt.xlabel('VE Predito (%)')\\n    plt.ylabel('Res√≠duos')\\n    plt.title(f'An√°lise de Res√≠duos - {best_name}')\\n    plt.grid(True, alpha=0.3)\\n    \\n    # Feature importance (Random Forest)\\n    if 'Random Forest' in model_results:\\n        rf_model = model_results['Random Forest']['model']\\n        feature_importance = rf_model.feature_importances_\\n        \\n        plt.subplot(2, 3, 5)\\n        # Top 10 features mais importantes\\n        top_indices = np.argsort(feature_importance)[-10:]\\n        top_features = [features[i] for i in top_indices]\\n        top_importance = feature_importance[top_indices]\\n        \\n        plt.barh(range(len(top_features)), top_importance)\\n        plt.yticks(range(len(top_features)), [f.replace('_', '\\\\n') for f in top_features])\\n        plt.xlabel('Import√¢ncia')\\n        plt.title('Top 10 Features Importantes (RF)')\\n        plt.grid(True, alpha=0.3)\\n    \\n    # Distribui√ß√£o dos erros\\n    plt.subplot(2, 3, 6)\\n    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\\n    plt.axvline(residuals.mean(), color='red', linestyle='--', alpha=0.7, \\n               label=f'M√©dia: {residuals.mean():.2f}')\\n    plt.xlabel('Res√≠duos')\\n    plt.ylabel('Frequ√™ncia')\\n    plt.title('Distribui√ß√£o dos Res√≠duos')\\n    plt.legend()\\n    plt.grid(True, alpha=0.3)\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    return model_results, scaler, (X_train, X_val, X_test, y_train, y_val, y_test), best_model\\n\\n# Executa treinamento\\nif df_ml_ready is not None and len(feature_list) > 0:\\n    model_results, scaler, data_splits, best_model = train_ve_prediction_models(df_ml_ready, feature_list)\\n    \\n    print(f\\\"\\\\n‚úÖ Treinamento conclu√≠do!\\\")\\n    print(f\\\"üéØ Melhor modelo: {best_model[0]}\\\")\\n    print(f\\\"üìä Features utilizadas: {len(feature_list)}\\\")\\nelse:\\n    print(\\\"‚ùå Dados n√£o dispon√≠veis para treinamento\\\")\\n    model_results, scaler, data_splits, best_model = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bec8c1",
   "metadata": {},
   "source": [
    "## 7. VE Adjustment Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o de predi√ß√£o de ajustes VE\\nclass VE_Optimizer:\\n    \\\"\\\"\\\"Classe para otimiza√ß√£o de VE usando modelo treinado\\\"\\\"\\\"\\n    \\n    def __init__(self, model, scaler, feature_list, rpm_bins, load_bins, ve_table):\\n        self.model = model\\n        self.scaler = scaler\\n        self.feature_list = feature_list\\n        self.rpm_bins = rpm_bins\\n        self.load_bins = load_bins\\n        self.ve_table = ve_table\\n    \\n    def predict_ve_adjustment(self, rpm, fuel_load, lambda_measured, \\n                            coolant_temp=90, intake_temp=25, timing_advance=25):\\n        \\\"\\\"\\\"Prediz ajuste VE para condi√ß√µes espec√≠ficas\\\"\\\"\\\"\\n        \\n        # Calcula VE atual da tabela\\n        current_ve = self._interpolate_ve(rpm, fuel_load)\\n        \\n        # Calcula lambda target\\n        lambda_target = self._calculate_lambda_target(rpm, fuel_load)\\n        \\n        # Cria features para predi√ß√£o\\n        features_dict = {\\n            'RPM': rpm,\\n            'FuelLoad': fuel_load,\\n            'VE_Current': current_ve,\\n            'Lambda_Delayed': lambda_measured,\\n            'Lambda_Target': lambda_target,\\n            'COOLANT_TEMP': coolant_temp,\\n            'INTAKE_TEMP': intake_temp,\\n            'TIMING_ADVANCE': timing_advance\\n        }\\n        \\n        # Features derivadas\\n        features_dict['RPM_Load_Interaction'] = rpm * fuel_load / 1000\\n        features_dict['VE_Lambda_Interaction'] = current_ve * lambda_measured\\n        features_dict['RPM_Trend'] = rpm  # Simplificado\\n        features_dict['Load_Trend'] = fuel_load\\n        features_dict['Lambda_Trend'] = lambda_measured\\n        features_dict['Lambda_Efficiency'] = 1 - abs(lambda_measured - lambda_target) / lambda_target\\n        features_dict['Mean_Piston_Speed'] = rpm * 0.1\\n        features_dict['Air_Density_Factor'] = 1 / (intake_temp + 273.15)\\n        features_dict['Specific_Load'] = fuel_load / (rpm / 1000)\\n        \\n        # Features categ√≥ricas (simplificado)\\n        features_dict.update({\\n            'RPM_Idle': 1 if rpm < 1500 else 0,\\n            'RPM_Low': 1 if 1500 <= rpm < 3000 else 0,\\n            'RPM_Mid': 1 if 3000 <= rpm < 5000 else 0,\\n            'RPM_High': 1 if rpm >= 5000 else 0,\\n            'Load_Light': 1 if fuel_load < 30 else 0,\\n            'Load_Moderate': 1 if 30 <= fuel_load < 70 else 0,\\n            'Load_Heavy': 1 if 70 <= fuel_load < 100 else 0,\\n            'Load_WOT': 1 if fuel_load >= 100 else 0\\n        })\\n        \\n        # Features de posi√ß√£o na grid\\n        if self.rpm_bins is not None and self.load_bins is not None:\\n            features_dict['RPM_Grid_Position'] = np.interp(rpm, self.rpm_bins, np.linspace(0, 1, len(self.rpm_bins)))\\n            features_dict['Load_Grid_Position'] = np.interp(fuel_load, self.load_bins, np.linspace(0, 1, len(self.load_bins)))\\n        \\n        # Cria array de features na ordem correta\\n        feature_values = []\\n        for feature in self.feature_list:\\n            if feature in features_dict:\\n                feature_values.append(features_dict[feature])\\n            else:\\n                feature_values.append(0)  # Valor padr√£o para features ausentes\\n        \\n        # Normaliza features\\n        features_array = np.array(feature_values).reshape(1, -1)\\n        features_scaled = self.scaler.transform(features_array)\\n        \\n        # Predi√ß√£o\\n        predicted_ve = self.model.predict(features_scaled)[0]\\n        \\n        # Aplicar limites de seguran√ßa (¬±25% do valor original)\\n        min_ve = current_ve * 0.75\\n        max_ve = current_ve * 1.25\\n        predicted_ve = np.clip(predicted_ve, min_ve, max_ve)\\n        \\n        # Calcula estat√≠sticas\\n        correction_factor = predicted_ve / current_ve\\n        percent_change = (correction_factor - 1) * 100\\n        lambda_error = lambda_measured - lambda_target\\n        \\n        return {\\n            'current_ve': current_ve,\\n            'predicted_ve': predicted_ve,\\n            'correction_factor': correction_factor,\\n            'percent_change': percent_change,\\n            'lambda_measured': lambda_measured,\\n            'lambda_target': lambda_target,\\n            'lambda_error': lambda_error,\\n            'safety_limited': predicted_ve != self.model.predict(features_scaled)[0]\\n        }\\n    \\n    def _interpolate_ve(self, rpm, fuel_load):\\n        \\\"\\\"\\\"Interpola VE da tabela atual\\\"\\\"\\\"\\n        if self.ve_table is None:\\n            return 50.0\\n        \\n        interp_func = interpolate.interp2d(self.rpm_bins, self.load_bins, self.ve_table, kind='linear')\\n        rpm_clipped = np.clip(rpm, min(self.rpm_bins), max(self.rpm_bins))\\n        load_clipped = np.clip(fuel_load, min(self.load_bins), max(self.load_bins))\\n        \\n        return float(interp_func(rpm_clipped, load_clipped)[0])\\n    \\n    def _calculate_lambda_target(self, rpm, fuel_load):\\n        \\\"\\\"\\\"Calcula lambda target baseado em condi√ß√µes\\\"\\\"\\\"\\n        if fuel_load < 25:\\n            return 1.0\\n        elif fuel_load < 60:\\n            return 0.95\\n        elif fuel_load < 80:\\n            return 0.88\\n        else:\\n            return 0.85\\n    \\n    def generate_optimized_table(self, test_conditions=None):\\n        \\\"\\\"\\\"Gera nova tabela VE otimizada\\\"\\\"\\\"\\n        \\n        print(\\\"üîß Gerando tabela VE otimizada...\\\")\\n        \\n        new_table = self.ve_table.copy()\\n        adjustments_made = 0\\n        \\n        # Para cada c√©lula da tabela\\n        for i, load in enumerate(self.load_bins):\\n            for j, rpm in enumerate(self.rpm_bins):\\n                \\n                # Simula leitura lambda (baseada no erro t√≠pico observado)\\n                current_ve = self.ve_table[i, j]\\n                lambda_target = self._calculate_lambda_target(rpm, load)\\n                \\n                # Simula lambda atual (com erro t√≠pico)\\n                # Em uma implementa√ß√£o real, isso viria dos dados de log\\n                lambda_measured = lambda_target + np.random.normal(0, 0.05)  # Erro simulado\\n                \\n                # Prediz novo VE\\n                prediction = self.predict_ve_adjustment(rpm, load, lambda_measured)\\n                \\n                # Aplica ajuste se significativo (>2%)\\n                if abs(prediction['percent_change']) > 2.0:\\n                    new_table[i, j] = prediction['predicted_ve']\\n                    adjustments_made += 1\\n        \\n        total_cells = len(self.load_bins) * len(self.rpm_bins)\\n        print(f\\\"üìä Ajustes aplicados: {adjustments_made}/{total_cells} c√©lulas ({adjustments_made/total_cells*100:.1f}%)\\\")\\n        \\n        return new_table\\n\\n# Cria otimizador se modelo estiver dispon√≠vel\\nif best_model is not None and scaler is not None:\\n    optimizer = VE_Optimizer(\\n        model=best_model[1],\\n        scaler=scaler,\\n        feature_list=feature_list,\\n        rpm_bins=rpm_bins,\\n        load_bins=load_bins,\\n        ve_table=ve_table\\n    )\\n    \\n    print(\\\"‚úÖ Otimizador VE criado com sucesso!\\\")\\n    \\n    # Exemplo de uso\\n    print(\\\"\\\\nüß™ Testando predi√ß√£o de VE:\\\")\\n    \\n    test_cases = [\\n        {'rpm': 2000, 'fuel_load': 40, 'lambda_measured': 0.98, 'description': 'Cruise (lambda rico)'},\\n        {'rpm': 3500, 'fuel_load': 75, 'lambda_measured': 0.91, 'description': 'Acelera√ß√£o (lambda OK)'},\\n        {'rpm': 5000, 'fuel_load': 95, 'lambda_measured': 0.82, 'description': 'WOT (lambda muito rico)'},\\n        {'rpm': 1200, 'fuel_load': 20, 'lambda_measured': 1.05, 'description': 'Marcha lenta (lambda pobre)'}\\n    ]\\n    \\n    for i, test in enumerate(test_cases, 1):\\n        result = optimizer.predict_ve_adjustment(\\n            rpm=test['rpm'],\\n            fuel_load=test['fuel_load'],\\n            lambda_measured=test['lambda_measured']\\n        )\\n        \\n        print(f\\\"\\\\n  Teste {i}: {test['description']}\\\")\\n        print(f\\\"    RPM: {test['rpm']}, Load: {test['fuel_load']} kPa\\\")\\n        print(f\\\"    Lambda: {test['lambda_measured']:.3f} (target: {result['lambda_target']:.3f})\\\")\\n        print(f\\\"    VE atual: {result['current_ve']:.1f}%\\\")\\n        print(f\\\"    VE predito: {result['predicted_ve']:.1f}%\\\")\\n        print(f\\\"    Mudan√ßa: {result['percent_change']:+.1f}%\\\")\\n        if result['safety_limited']:\\n            print(f\\\"    ‚ö†Ô∏è  Limitado por seguran√ßa\\\")\\nelse:\\n    print(\\\"‚ùå Modelo n√£o dispon√≠vel para criar otimizador\\\")\\n    optimizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f3dd6",
   "metadata": {},
   "source": [
    "## 8. Results Visualization and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f726bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o e valida√ß√£o dos resultados finais\\nif optimizer is not None:\\n    print(\\\"üé® Gerando visualiza√ß√µes finais e valida√ß√£o...\\\")\\n    \\n    # Gera tabela VE otimizada\\n    optimized_table = optimizer.generate_optimized_table()\\n    \\n    # Visualiza√ß√µes comparativas\\n    fig, axes = plt.subplots(2, 3, figsize=(22, 14))\\n    \\n    # 1. Tabela VE Original\\n    im1 = axes[0,0].imshow(ve_table, aspect='auto', origin='lower', cmap='RdYlBu_r', vmin=0, vmax=100)\\n    axes[0,0].set_title('Tabela VE Original', fontsize=14, fontweight='bold')\\n    axes[0,0].set_xlabel('RPM Bins')\\n    axes[0,0].set_ylabel('Load Bins (kPa)')\\n    \\n    # Configurar ticks\\n    rpm_ticks = np.arange(0, len(rpm_bins), 2)\\n    load_ticks = np.arange(0, len(load_bins), 2)\\n    axes[0,0].set_xticks(rpm_ticks)\\n    axes[0,0].set_xticklabels([f'{rpm_bins[i]:.0f}' for i in rpm_ticks], rotation=45)\\n    axes[0,0].set_yticks(load_ticks)\\n    axes[0,0].set_yticklabels([f'{load_bins[i]:.0f}' for i in load_ticks])\\n    \\n    cbar1 = plt.colorbar(im1, ax=axes[0,0], shrink=0.8)\\n    cbar1.set_label('VE (%)', rotation=270, labelpad=15)\\n    \\n    # 2. Tabela VE Otimizada\\n    im2 = axes[0,1].imshow(optimized_table, aspect='auto', origin='lower', cmap='RdYlBu_r', vmin=0, vmax=100)\\n    axes[0,1].set_title('Tabela VE Otimizada (IA)', fontsize=14, fontweight='bold')\\n    axes[0,1].set_xlabel('RPM Bins')\\n    axes[0,1].set_ylabel('Load Bins (kPa)')\\n    axes[0,1].set_xticks(rpm_ticks)\\n    axes[0,1].set_xticklabels([f'{rpm_bins[i]:.0f}' for i in rpm_ticks], rotation=45)\\n    axes[0,1].set_yticks(load_ticks)\\n    axes[0,1].set_yticklabels([f'{load_bins[i]:.0f}' for i in load_ticks])\\n    \\n    cbar2 = plt.colorbar(im2, ax=axes[0,1], shrink=0.8)\\n    cbar2.set_label('VE (%)', rotation=270, labelpad=15)\\n    \\n    # 3. Diferen√ßa (Otimizada - Original)\\n    diff_table = optimized_table - ve_table\\n    im3 = axes[0,2].imshow(diff_table, aspect='auto', origin='lower', cmap='RdBu_r', \\n                          vmin=-10, vmax=10)\\n    axes[0,2].set_title('Diferen√ßa (Otimizada - Original)', fontsize=14, fontweight='bold')\\n    axes[0,2].set_xlabel('RPM Bins')\\n    axes[0,2].set_ylabel('Load Bins (kPa)')\\n    axes[0,2].set_xticks(rpm_ticks)\\n    axes[0,2].set_xticklabels([f'{rpm_bins[i]:.0f}' for i in rpm_ticks], rotation=45)\\n    axes[0,2].set_yticks(load_ticks)\\n    axes[0,2].set_yticklabels([f'{load_bins[i]:.0f}' for i in load_ticks])\\n    \\n    cbar3 = plt.colorbar(im3, ax=axes[0,2], shrink=0.8)\\n    cbar3.set_label('Œî VE (%)', rotation=270, labelpad=15)\\n    \\n    # 4. Distribui√ß√£o das mudan√ßas\\n    axes[1,0].hist(diff_table.flatten(), bins=30, alpha=0.7, edgecolor='black', color='skyblue')\\n    axes[1,0].axvline(0, color='red', linestyle='--', alpha=0.7, linewidth=2)\\n    axes[1,0].set_xlabel('Mudan√ßa VE (%)')\\n    axes[1,0].set_ylabel('Frequ√™ncia')\\n    axes[1,0].set_title('Distribui√ß√£o das Mudan√ßas VE')\\n    axes[1,0].grid(True, alpha=0.3)\\n    \\n    # Estat√≠sticas\\n    mean_change = diff_table.mean()\\n    std_change = diff_table.std()\\n    max_increase = diff_table.max()\\n    max_decrease = diff_table.min()\\n    \\n    axes[1,0].text(0.02, 0.98, f'M√©dia: {mean_change:.2f}%\\\\nStd: {std_change:.2f}%\\\\nMax+: {max_increase:.1f}%\\\\nMax-: {max_decrease:.1f}%',\\n                  transform=axes[1,0].transAxes, verticalalignment='top',\\n                  bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\\n    \\n    # 5. An√°lise por zona de RPM\\n    rpm_zones = {\\n        'Baixo (500-2000)': (0, 6),\\n        'M√©dio (2000-4000)': (6, 12),\\n        'Alto (4000+)': (12, 16)\\n    }\\n    \\n    zone_changes = []\\n    zone_labels = []\\n    \\n    for zone_name, (start_idx, end_idx) in rpm_zones.items():\\n        zone_diff = diff_table[:, start_idx:end_idx]\\n        zone_changes.extend(zone_diff.flatten())\\n        zone_labels.extend([zone_name] * zone_diff.size)\\n    \\n    # Boxplot por zona\\n    zone_data = []\\n    for zone_name, (start_idx, end_idx) in rpm_zones.items():\\n        zone_diff = diff_table[:, start_idx:end_idx]\\n        zone_data.append(zone_diff.flatten())\\n    \\n    bp = axes[1,1].boxplot(zone_data, labels=list(rpm_zones.keys()), patch_artist=True)\\n    axes[1,1].set_ylabel('Mudan√ßa VE (%)')\\n    axes[1,1].set_title('Mudan√ßas VE por Zona de RPM')\\n    axes[1,1].grid(True, alpha=0.3)\\n    axes[1,1].axhline(0, color='red', linestyle='--', alpha=0.7)\\n    \\n    # Cores para boxplot\\n    colors = ['lightblue', 'lightgreen', 'lightcoral']\\n    for patch, color in zip(bp['boxes'], colors):\\n        patch.set_facecolor(color)\\n        patch.set_alpha(0.7)\\n    \\n    # 6. Mapa de confian√ßa (baseado na densidade de dados do log)\\n    if 'df_ml_ready' in locals() and df_ml_ready is not None:\\n        # Cria grid de confian√ßa baseado na quantidade de dados\\n        confidence_map = np.zeros_like(ve_table)\\n        \\n        for i, load in enumerate(load_bins):\\n            for j, rpm in enumerate(rpm_bins):\\n                # Conta pontos pr√≥ximos a esta c√©lula\\n                nearby_points = df_ml_ready[\\n                    (abs(df_ml_ready['RPM'] - rpm) < 200) & \\n                    (abs(df_ml_ready['FuelLoad'] - load) < 10)\\n                ]\\n                \\n                # Confian√ßa baseada no n√∫mero de pontos\\n                confidence_map[i, j] = min(len(nearby_points) / 50, 1.0)  # Normaliza para 0-1\\n        \\n        im6 = axes[1,2].imshow(confidence_map, aspect='auto', origin='lower', \\n                              cmap='Greens', vmin=0, vmax=1)\\n        axes[1,2].set_title('Mapa de Confian√ßa\\\\n(baseado em densidade de dados)', fontsize=14, fontweight='bold')\\n        axes[1,2].set_xlabel('RPM Bins')\\n        axes[1,2].set_ylabel('Load Bins (kPa)')\\n        axes[1,2].set_xticks(rpm_ticks)\\n        axes[1,2].set_xticklabels([f'{rpm_bins[i]:.0f}' for i in rpm_ticks], rotation=45)\\n        axes[1,2].set_yticks(load_ticks)\\n        axes[1,2].set_yticklabels([f'{load_bins[i]:.0f}' for i in load_ticks])\\n        \\n        cbar6 = plt.colorbar(im6, ax=axes[1,2], shrink=0.8)\\n        cbar6.set_label('Confian√ßa', rotation=270, labelpad=15)\\n    else:\\n        # Se n√£o tiver dados, mostra placeholder\\n        axes[1,2].text(0.5, 0.5, 'Mapa de Confian√ßa\\\\n(dados n√£o dispon√≠veis)', \\n                      ha='center', va='center', transform=axes[1,2].transAxes,\\n                      fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\\n        axes[1,2].set_title('Mapa de Confian√ßa')\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    # Relat√≥rio final\\n    print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n    print(\\\"üìä RELAT√ìRIO FINAL DE OTIMIZA√á√ÉO VE\\\")\\n    print(\\\"=\\\"*60)\\n    \\n    total_cells = ve_table.size\\n    changed_cells = np.sum(np.abs(diff_table) > 0.1)  # Mudan√ßas > 0.1%\\n    significant_changes = np.sum(np.abs(diff_table) > 2.0)  # Mudan√ßas > 2%\\n    \\n    print(f\\\"üî¢ Total de c√©lulas na tabela: {total_cells}\\\")\\n    print(f\\\"üìà C√©lulas modificadas (>0.1%): {changed_cells} ({changed_cells/total_cells*100:.1f}%)\\\")\\n    print(f\\\"‚ö° Mudan√ßas significativas (>2%): {significant_changes} ({significant_changes/total_cells*100:.1f}%)\\\")\\n    print(f\\\"üìä Mudan√ßa m√©dia: {mean_change:.2f}% ¬± {std_change:.2f}%\\\")\\n    print(f\\\"üìà Maior aumento: +{max_increase:.1f}%\\\")\\n    print(f\\\"üìâ Maior redu√ß√£o: {max_decrease:.1f}%\\\")\\n    \\n    # An√°lise por zona de carga\\n    print(f\\\"\\\\nüéØ An√°lise por zona de carga:\\\")\\n    load_zones = [\\n        ('Baixa (0-40 kPa)', 0, 7),\\n        ('M√©dia (40-70 kPa)', 7, 13),\\n        ('Alta (70+ kPa)', 13, 16)\\n    ]\\n    \\n    for zone_name, start_idx, end_idx in load_zones:\\n        zone_diff = diff_table[start_idx:end_idx, :]\\n        zone_mean = zone_diff.mean()\\n        zone_std = zone_diff.std()\\n        print(f\\\"  {zone_name}: {zone_mean:+.2f}% ¬± {zone_std:.2f}%\\\")\\n    \\n    print(f\\\"\\\\nü§ñ Modelo utilizado: {best_model[0]}\\\")\\n    if 'estimated_delay' in locals():\\n        print(f\\\"‚è±Ô∏è  Delay da sonda estimado: {estimated_delay:.2f}s\\\")\\n    \\n    print(f\\\"\\\\nüíæ Para aplicar esta tabela otimizada:\\\")\\n    print(f\\\"   1. Revise as mudan√ßas significativas\\\")\\n    print(f\\\"   2. Teste em condi√ß√µes controladas\\\")\\n    print(f\\\"   3. Monitore lambda em tempo real\\\")\\n    print(f\\\"   4. Fa√ßa ajustes incrementais se necess√°rio\\\")\\n    \\n    print(f\\\"\\\\n‚ö†Ô∏è  IMPORTANTE: Sempre monitore temperatura dos gases de escape\\\")\\n    print(f\\\"    e par√¢metros de detona√ß√£o ao aplicar mudan√ßas significativas!\\\")\\n    \\nelse:\\n    print(\\\"‚ùå Otimizador n√£o dispon√≠vel para gerar resultados finais\\\")\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*60)\\nprint(\\\"‚úÖ AN√ÅLISE DE OTIMIZA√á√ÉO VE CONCLU√çDA!\\\")\\nprint(\\\"=\\\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OBD2 VE Optimizer",
   "language": "python",
   "name": "obd2_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
